<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <title>Cloud9: A Hadoop toolkit for working with big data</title>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="">
    <meta name="author" content="">

    <!-- Le styles -->
    <link href="../assets/css/bootstrap.css" rel="stylesheet">
    <link href="../assets/css/bootstrap-responsive.css" rel="stylesheet">
    <link href="../assets/css/docs.css" rel="stylesheet">
    <link href="../assets/js/google-code-prettify/prettify.css" rel="stylesheet">

    <!-- Le HTML5 shim, for IE6-8 support of HTML5 elements -->
    <!--[if lt IE 9]>
      <script src="http://html5shim.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->

  </head>

  <body data-spy="scroll" data-target=".bs-docs-sidebar">

    <!-- Navbar
    ================================================== -->
    <div class="navbar navbar-inverse navbar-fixed-top">
      <div class="navbar-inner">
        <div class="container">
          <button type="button" class="btn btn-navbar" data-toggle="collapse" data-target=".nav-collapse">
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
          </button>
          <div class="nav-collapse collapse">
            <ul class="nav">
              <li class="">
                <a href="../../index.html">Home</a>
              </li>
              <li class="">
                <a href="../contents.html">Table of Contents</a>
              </li>
              <li class="">
                <a href="../api/index.html">API</a>
              </li>
            </ul>
          </div>
        </div>
      </div>
    </div>

<!-- Subhead
================================================== -->
<header class="jumbotron subhead" id="overview">
  <div class="container">
    <h1>Cloud<sup>9</sup></h1>
    <p class="lead">A Hadoop toolkit for working with big data</p>
  </div>
</header>

  <div class="container">

<div class="page-header">
<h2>Working with Wikipedia</h2>
</div>

<p>For several reasons, Wikipedia is among the most popular datasets
to process with Hadoop: it's big, it contains a lot of text, and it
has a rich link structure.  And perhaps most important of all,
Wikipedia can be freely downloaded by anyone.  Where do you get it?
See <a href="http://en.wikipedia.org/wiki/Wikipedia_database">this
page</a> for information about raw Wikipedia dumps in XML.  Direct
access to English Wikipedia dumps can be
found <a href="http://download.wikimedia.org/enwiki/">here</a>.</p>

<p>In this guide, we'll be working with the XML dump of English
Wikipedia dated May 3, 2013
(<code>enwiki-20130503-pages-articles.xml.bz2</code>).  Note that
Wikipedia is distributed as a single, large XML file compressed with
bzip2. We'll want to repack the dataset as
block-compressed <code>SequenceFile</code>s (see below), so you should
uncompress the file before putting it into HDFS.</p>

<p>As a sanity check, the first thing you might want to do is count
how many pages there are in this particular version of Wikipedia:</p>

<pre class="code">
etc/hadoop-cluster.sh edu.umd.cloud9.collection.wikipedia.CountWikipediaPages \
  -input /shared/collections/wikipedia/raw/enwiki-20130503-pages-articles.xml -wiki_language en
</pre>

<p>Conveniently enough, Cloud<sup><small>9</small></sup> provides a
<code>WikipediaPageInputFormat</code> that automatically parses the
XML dump file, so that your mapper will be
presented <code>WikipediaPage</code> objects to process.</p>

<p>After running the above program, you'll see the follow statistics
stored in counters:</p>

<table class="table" style="width: 500px;">
<tr><td><b>Type</b></td>
    <td style="text-align:right;"><b>Count</b></td>
</tr>

<tr>
<td>Redirect pages</td>
<td style="text-align:right;">5,998,969</td>
</tr>

<tr>
<td>Disambiguation pages</td>
<td style="text-align:right;">126,046</td>
</tr>

<tr>
<td>Empty pages</td>
<td style="text-align:right;">566</td>
</tr>

<tr>
<td>Stub articles</td>
<td style="text-align:right;">1,740,514</td>
</tr>

<tr>
<td>Total articles (including stubs)</td>
<td style="text-align:right;">4,293,113</td>
</tr>

<tr>
<td>Other</td>
<td style="text-align:right;">3,031,844</td>
</tr>

<tr>
<td><b>Total</b></td>
<td style="text-align:right;">13,450,538</td>
</tr>

</table>

<p>So far, so good!</p>

<p>The remainder of this guide will provide a more principled approach
to working with Wikipedia.  However, for something "quick-and-dirty",
we can dump all articles into a flat text file with the following
command:</p>

<pre class="code">
etc/hadoop-cluster.sh edu.umd.cloud9.collection.wikipedia.DumpWikipediaToPlainText \
  -input /shared/collections/wikipedia/raw/enwiki-20130503-pages-articles.xml -wiki_language en -output enwiki-20130503.txt
</pre>

<p>The output consist of text files, one article per line (article
title and contents, separated by a tab). Note
that <code>DumpWikipediaToPlainText</code> discards all
non-articles.</p>

<h2>Sequentially-Numbered Docnos</h2>

<p>Many information retrieval and other text processing tasks require
that all documents in the collection be sequentially numbered, from 1
... <i>n</i>.  Typically, you'll want to start with document 1 as
opposed to 0 because it is not possible to represent 0 with many
standard compression schemes used in information retrieval (i.e.,
gamma codes).</p>

<p>The next step is to create a mapping from Wikipedia internal
document ids (docids) to these sequentially-numbered ids (docnos).
Although, in general, docids can be arbitrary alphanumeric strings,
we'll be using Wikipedia internal ids (which happend to be integers,
but we'll treat as strings) as the docids.  This is a bit confusing,
so we'll illustrate.  The first full article in our XML dump file is:</p>

<pre class="code">
  ...
  &lt;page&gt;
    &lt;title&gt;Anarchism&lt;/title&gt;
    &lt;ns&gt;0&lt;/ns&gt;
    &lt;id&gt;12&lt;/id&gt;
  ...
</pre>

<p>So this page gets docid "12" (the Wikipedia internal id) and docno
1 (the sequentially-numbered id).  We can build the mapping between
docids and docnos by the following command:</p>

<pre class="code">
etc/hadoop-cluster.sh edu.umd.cloud9.collection.wikipedia.WikipediaDocnoMappingBuilder \
  -input /shared/collections/wikipedia/raw/enwiki-20130503-pages-articles.xml \
  -output_file enwiki-20130503-docno.dat -wiki_language en -keep_all 
</pre>

<p>The mapping file <code>enwiki-20130503-docno.dat</code> is used by the
class <code>WikipediaDocnoMapping</code>, which provides a nice API
for mapping back and forth between docnos and docids. Note that in
this process, we're discarding all non-articles.</p>

<p>Aside: Why not use the article titles are docids?  We could, but
since we need to maintain the docid to docno mapping in memory (for
fast lookup), using article titles as docids would be expensive (lots
of strings to store in memory).</p>

<p>Note that by default, <code>WikipediaDocnoMappingBuilder</code>
discards non-articles (e.g., disambiguation pages, redirects,
etc.). To retain all pages, use the <code>-keep_all</code> option.</p>

<h2>Repacking the Records</h2>

<p>So far, we've been directly processing the raw uncompressed
Wikipedia data dump in XML.  To gain the benefits of compression
(within the Hadoop framework), we'll now repack the XML file into
block-compressed <code>SequenceFile</code>s, where the keys are the docnos, and the
values are <code>WikipediaPage</code> objects.  This will make subsequent
processing much easier.</p>

<p>Here's the command-line invocation:</p>

<pre class="code">
etc/hadoop-cluster.sh edu.umd.cloud9.collection.wikipedia.RepackWikipedia \
  -input /shared/collections/wikipedia/raw/enwiki-20130503-pages-articles.xml \
  -output enwiki-20130503.block -wiki_language en \
  -mapping_file enwiki-20130503-docno.dat -compression_type block
</pre>

<p>In addition to block-compression, the program <code>RepackWikipedia</code> also
supports record-level compression as well as no compression:</p>

<pre class="code">
# Record-level compression
etc/hadoop-cluster.sh edu.umd.cloud9.collection.wikipedia.RepackWikipedia \
  -input /shared/collections/wikipedia/raw/enwiki-20130503-pages-articles.xml \
  -output enwiki-20130503.record -wiki_language en \
  -mapping_file enwiki-20130503-docno.dat -compression_type record

# No compression
etc/hadoop-cluster.sh edu.umd.cloud9.collection.wikipedia.RepackWikipedia \
  -input /shared/collections/wikipedia/raw/enwiki-20130503-pages-articles.xml \
  -output enwiki-20130503.seq -wiki_language en \
  -mapping_file enwiki-20130503-docno.dat -compression_type none
</pre>

<p>Here's how the various methods stack up:</p>

<table class="table" style="width: 500px;">
<tr><td><b>Format</b></td>
    <td style="text-align:right;"><b>Size (MB)</b></td>
</tr>

<tr>
<td align="left">Original XML dump (bzip2-compressed)</td>
<td style="text-align:right;">9,814</td>
</tr>

<tr>
<td align="left">Original XML dump (uncompressed)</td>
<td style="text-align:right;">43,777</td>
</tr>

<tr>
<td align="left"><code>SequenceFile</code> (block-compressed)</td>
<td style="text-align:right;">12,463</td>
</tr>

<tr>
<td align="left"><code>SequenceFile</code> (record-compressed)</td>
<td style="text-align:right;">17,523</td>
</tr>

<tr>
<td align="left"><code>SequenceFile</code> (no compression)</td>
<td style="text-align:right;">44,120</td>
</tr>

</table>

<p>Block-compressed <code>SequenceFile</code>s are fairly space
efficient. Record-compressed <code>SequenceFile</code>s are less
space-efficient, but retain the ability to directly seek to any
<code>WikipediaPage</code> object (whereas with block compression you
can only seek to block boundaries). In most usage scenarios,
block-compressed <code>SequenceFile</code>s are preferred.</p>

<h2>Supporting Random Access</h2>

<p>Cloud<sup><small>9</small></sup> provides utilities for random
access to Wikipedia articles.  First, we need to build a forward
index:</p>

<pre class="code">
etc/hadoop-cluster.sh edu.umd.cloud9.collection.wikipedia.WikipediaForwardIndexBuilder \
  -input /user/jimmylin/enwiki-20130503.block -index_file enwiki-20130503.findex.dat
</pre>

<p><b>Note:</b> It mostly doesn't matter elsewhere, but
here <code>-input</code> must take an absolute path.</p>

<p>With this forward index, we can now programmatically access
Wikipedia articles given docno or docid.  Here's a snippet of code
that does this:</p>

<pre class="code">
Configuration conf = getConf();
WikipediaForwardIndex f = new WikipediaForwardIndex(conf);
f.loadIndex(new Path("enwiki-20130503.findex.dat"), new Path("enwiki-20130503-docno.dat"), FileSystem.get(conf));

WikipediaPage page;

// fetch docno
page = f.getDocument(1000);
System.out.println(page.getDocid() + ": " + page.getTitle());

// fetch docid
page = f.getDocument("1875");
System.out.println(page.getDocid() + ": " + page.getTitle());
</pre>

<p>This is illustrated by a simple command-line program that allows
you to find out the title of a page given either the docno or the
docid:</p>

<pre class="code">
etc/hadoop-cluster.sh edu.umd.cloud9.collection.wikipedia.LookupWikipediaArticle \
  enwiki-20130503.findex.dat enwiki-20130503-docno.dat
</pre>

<p>Finally, there's also a web interface for browsing, which can be
started with the following invocation:</p>

<pre class="code">
etc/hadoop-cluster.sh edu.umd.cloud9.collection.DocumentForwardIndexHttpServer \
  -docnoMapping enwiki-20130503-docno.dat -index enwiki-20130503.findex.dat
</pre>

<p>When the server starts up, it'll report back the host information
of the node it's running on (along with the port). You can then direct
your browser at the relevant URL and gain access to the webapp.</p>

  </div>



    <!-- Footer
    ================================================== -->
    <footer class="footer">
      <div class="container">
        <p class="pull-right"><a href="#">Back to top</a></p>
        <p>Designed using <a href="http://twitter.github.com/bootstrap/">Bootstrap</a>.</p>
        <p>Code licensed under <a href="http://www.apache.org/licenses/LICENSE-2.0" target="_blank">Apache License v2.0</a>, documentation under <a href="http://creativecommons.org/licenses/by/3.0/">CC BY 3.0</a>.</p>
      </div>
    </footer>

    <!-- Le javascript
    ================================================== -->
    <!-- Placed at the end of the document so the pages load faster -->
    <script src="../assets/js/jquery.js"></script>
    <script src="../assets/js/google-code-prettify/prettify.js"></script>
    <script src="../assets/js/bootstrap-transition.js"></script>
    <script src="../assets/js/bootstrap-alert.js"></script>
    <script src="../assets/js/bootstrap-modal.js"></script>
    <script src="../assets/js/bootstrap-dropdown.js"></script>
    <script src="../assets/js/bootstrap-scrollspy.js"></script>
    <script src="../assets/js/bootstrap-tab.js"></script>
    <script src="../assets/js/bootstrap-tooltip.js"></script>
    <script src="../assets/js/bootstrap-popover.js"></script>
    <script src="../assets/js/bootstrap-button.js"></script>
    <script src="../assets/js/bootstrap-collapse.js"></script>
    <script src="../assets/js/bootstrap-carousel.js"></script>
    <script src="../assets/js/bootstrap-typeahead.js"></script>
    <script src="../assets/js/bootstrap-affix.js"></script>

  </body>
</html>
